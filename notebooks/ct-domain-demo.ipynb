{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Phishing domains with UniSim\n",
    "\n",
    "This colab showcases how to use UniSim near-duplicate text embeddings to find look-a-like phishing domains.\n",
    "\n",
    "This technique can be used to monitor Certificate Transparency logs to find look-a-like phishing domains in near-realtime. If you are interested to an end-to-end implementation of this idea you will find a complete server side / client side implementation of this idea at [cthunter github repository](https://github.com/ebursztein/cthunter).\n",
    "\n",
    "For this colab we are going to use a static list of domains and fake domains to demonstrate the underlying technical concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:ONNX is running on CPU - you need to install CUDA/onnxruntime-gpu to run on GPU\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import json\n",
    "from unisim import TextSim\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "First thing first, we are loading made-up data that simulate our use-case finding if the domains present in the logs look alike the domains we are monitoring.\n",
    "\n",
    "The `domains.json` file contains two list:\n",
    "\n",
    "- **domains**: this is a list of domains that are monitored\n",
    "- **logs**: those are the made-up domains seen in the logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Domains: ['google', 'facebook', 'twitter', 'linkedin', 'instagram']\n",
      "Some logs: ['gooolgle', 'g00gle', 'gùô§ogl', 'googglee', 'google']\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open('./data/domains.json'))\n",
    "domains = data['domains']\n",
    "logs = data['logs']\n",
    "print('Some Domains:', domains[:5])\n",
    "print(\"Some logs:\", logs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Unisim embedding works\n",
    "\n",
    "UniSim near-duplicate embedding is a very small transformer that leverage the RetVec encoder and metric learning to compute vectors that allows us to tell if two strings are closers to each-other.\n",
    "\n",
    "Let's demonstrate how this works in practice so we get a sense of the value returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Google and gùô§ogle is 0.8848875164985657\n",
      "Similarity between Google and googlee is 0.8945294618606567\n",
      "Similarity between Google and loogle is 0.7687983512878418\n",
      "Similarity between Google and thisisnotamatch is 0.4284525513648987\n"
     ]
    }
   ],
   "source": [
    "ts = TextSim()  # init UniSim text similarity\n",
    "test_domain = \"Google\"\n",
    "test_logs = [\"gùô§ogle\", 'googlee', 'loogle', \"thisisnotamatch\"]\n",
    "for log in test_logs:\n",
    "    s = ts.similarity(test_domain, log)\n",
    "    print(f\"Similarity between {test_domain} and {log} is {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible in the example above, the father the domains, the lower is the similarity so we need a threshold. 0.85 is usually a good starting point until you calibrate the value based on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and searching\n",
    "\n",
    "A key issue with traditional text similarity algorithms is that they have a $N^2$ in complexity which makes it prohibitively expensive to compute at scale.\n",
    "\n",
    "However UniSim transforms strings into vectors representation so we can take advantages of this in multiples ways to speed up computations:\n",
    "1. We can pre-compute the domains embeddings and only compute the match domains vectors as they arrive saving computation time.\n",
    "2. We can leverage GPU to do batch computation to compute domains embeddings leveraging GPU paralellization\n",
    "3. If the list of domains become very large we can use an Approximate Nearest Neighboor (ANN) algorithm to do search in $O(N)$\n",
    "\n",
    "Here our list of domains is quite small so we only going to use batching and use an exact SIMD accelerated search.  To do so we are going to perform three steps:\n",
    "\n",
    "1. Index our domains\n",
    "2. Create a batch of logs\n",
    "3. Look throught the results to find matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_threshold = 0.85\n",
    "ts.reset_index()   # reset index in case the cell is ran multiple times\n",
    "idxs = ts.add(domains)  # index the domains we want to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.index_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search took 0.17s\n",
      "Log        Match       Similarity\n",
      "---------  --------  ------------\n",
      "gooolgle   google        0.886348\n",
      "gùô§ogl      google        0.875465\n",
      "googglee   google        0.936674\n",
      "google     google        1\n",
      "goolgle    google        0.887597\n",
      "thegoogle  google        0.901965\n",
      "go0gl3     google        0.878208\n",
      "bookface   facebook      0.952432\n",
      "faceb0ok   facebook      0.965293\n",
      "twiter     twitter       0.954171\n",
      "twittter   twitter       0.98916\n",
      "twiiter    twitter       0.935433\n",
      "Eebay      ebay          0.917151\n",
      "ebayy      ebay          0.951734\n",
      "eb4y       ebay          0.899255\n",
      "amazoon    amazon        0.979886\n",
      "ammazonn   amazon        0.898796\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = ts.search(logs, k=2, similarity_threshold=match_threshold)  # match the logs against the dataset\n",
    "print(f\"Search took {time() - start:.2f}s\")\n",
    "# get matching matching results\n",
    "matches = []\n",
    "not_matches = []   # just to show what it looks like\n",
    "for result in results.results:\n",
    "    log_match = result.query_data\n",
    "    for match in result.matches:\n",
    "        if match.is_match:\n",
    "            matches.append((log_match, match.data, match.similarity))\n",
    "        else:\n",
    "            not_matches.append((log_match, match.data, match.similarity))\n",
    "print(tabulate(matches, headers=['Log', 'Match', 'Similarity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log           Match        Similarity\n",
      "------------  ---------  ------------\n",
      "gooolgle      facebook       0.533099\n",
      "g00gle        google         0.787087\n",
      "g00gle        apple          0.509571\n",
      "gùô§ogl         facebook       0.477082\n",
      "googglee      facebook       0.538006\n",
      "google        facebook       0.557381\n",
      "goolgle       facebook       0.525342\n",
      "goolge        google         0.741205\n",
      "goolge        facebook       0.526553\n",
      "goog-le.net   google         0.842378\n",
      "goog-le.net   facebook       0.499355\n",
      "thegoogle     facebook       0.526736\n",
      "G00gleLogin   google         0.701787\n",
      "G00gleLogin   youtube        0.461425\n",
      "go0gl3        facebook       0.549299\n",
      "bookface      google         0.58106\n",
      "faecbook      facebook       0.849847\n",
      "faecbook      google         0.555835\n",
      "faecbook      facebook       0.849847\n",
      "faecbook      google         0.555835\n",
      "faceb0ok      tiktok         0.510899\n",
      "f@ceb00k.bl   facebook       0.689414\n",
      "f@ceb00k.bl   ebay           0.564488\n",
      "faecbo-ok     facebook       0.798672\n",
      "faecbo-ok     google         0.544194\n",
      "myfaecbook    facebook       0.739721\n",
      "myfaecbook    google         0.520277\n",
      "ficebo0k.com  facebook       0.78167\n",
      "ficebo0k.com  flickr         0.580344\n",
      "amazin        amazon         0.721783\n",
      "amazin        instagram      0.537025\n",
      "theamazon     amazon         0.839523\n",
      "theamazon     youtube        0.529078\n",
      "myamazin      amazon         0.60986\n",
      "myamazin      instagram      0.521222\n",
      "twiter        twitch         0.781063\n",
      "twittter      twitch         0.742142\n",
      "tw1tt3r       twitter        0.748018\n",
      "tw1tt3r       twitch         0.659114\n",
      "twiiter       twitch         0.742873\n",
      "Eebay         etsy           0.57251\n",
      "ebayy         etsy           0.588192\n",
      "eb4y          etsy           0.664549\n",
      "3b4y          ebay           0.791513\n",
      "3b4y          etsy           0.60247\n",
      "paypall       apple          0.607724\n",
      "paypall       alibaba        0.507827\n",
      "palpay        apple          0.529323\n",
      "palpay        youtube        0.515057\n",
      "p4yoal        youtube        0.605816\n",
      "p4yoal        pinterest      0.569954\n",
      "p4ypal        apple          0.620549\n",
      "p4ypal        pinterest      0.543642\n",
      "p4ypa1        pinterest      0.5728\n",
      "p4ypa1        ebay           0.535126\n",
      "p4yp4l        pinterest      0.574862\n",
      "p4yp4l        apple          0.55518\n",
      "amazoon       youtube        0.564013\n",
      "Amazz0n       amazon         0.730493\n",
      "Amazz0n       telegram       0.499821\n",
      "amaz00n       amazon         0.791403\n",
      "amaz00n       telegram       0.523723\n",
      "amzùô§n         amazon         0.822412\n",
      "amzùô§n         telegram       0.535912\n",
      "ammazonn      youtube        0.50883\n"
     ]
    }
   ],
   "source": [
    "# let's display some non-matches to show what it looks like for the wrong pairs\n",
    "print(tabulate(not_matches, headers=['Log', 'Match', 'Similarity']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
